%% Based on a TeXnicCenter-Template by Gyorgy SZEIDL.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------
%
%\documentclass[10pt,a4paper]{exam}
\documentclass[11pt,a4paper,answers]{exam}
%
%----------------------------------------------------------
% This is a sample document for the standard LaTeX Book Class
% Class options
%       --  Body text point size:
%                        10pt (default), 11pt, 12pt
%       --  Paper size:  letterpaper (8.5x11 inch, default)
%                        a4paper, a5paper, b5paper,
%                        legalpaper, executivepaper
%       --  Orientation (portrait is the default):
%                        landscape
%       --  Printside:   oneside, twoside (default)
%       --  Quality:     final(default), draft
%       --  Title page:  titlepage, notitlepage
%       --  Columns:     onecolumn (default), twocolumn
%       --  Start chapter on left:
%                        openright(no, default), openany
%       --  Equation numbering (equation numbers on right is the default):
%                        leqno
%       --  Displayed equations (centered is the default):
%                        fleqn (flush left)
%       --  Open bibliography style (closed bibliography is the default):
%                        openbib
% For instance the command
%          \documentclass[a4paper,12pt,reqno]{book}
% ensures that the paper size is a4, fonts are typeset at the size 12p
% and the equation numbers are on the right side.
%
\usepackage{amsmath}%
\usepackage{amsthm}
\usepackage{amsfonts}%
\usepackage{amssymb}%
\usepackage{graphicx}
\usepackage{enumerate}
%----------------------------------------------------------
\newcommand{\IfSolution}[1]{#1}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}}
\renewcommand{\theenumiii}{\theenumii.\arabic{enumiii}}
\renewcommand{\qedsymbol}{$\blacksquare$}


\newtheorem{proposition}{Proposition}
\providecommand{\nline}{\vspace*{.5cm}}

\rhead{\textsf{CS3244-HW1}}
%\lhead{\textsf{Name: \underline{\hspace{1.2in}}}}


\global\long\def\qs{$\quad$}
\global\long\def\qss{$\quad\quad$}
\global\long\def\qsss{$\quad\quad\quad$}

%----------------------------------------------------------

\begin{document}

\begin{center}
{\bf CS3244 HW1}

{\bf A0113598X}
\end{center}

\nline

\begin{questions}
	\question
	Suppose that we use a perceptron to detect spam messages. Let's say that each email messages represented by the frequency of occurrence of keywords, and the output is +1 if the message is considered spam
	\begin{parts}
		\part Can you think of some keywords that will end up with a large positive weight into perceptron?
		\begin{solution}
			buy, sale, deal, discount, join, win, free, offer, order, earn
		\end{solution}
		\part How about keywords that will get a negative weight?
		\begin{solution}
			submit, deadline, announcement, compulsory, please, thanks, regards, kindly, reminder, sorry
		\end{solution}
		\part What parameter in the perceptron directly affects how many borderline messages end up classified as spam?
		\begin{solution}
			The bias $w_{0}$. As when the bias is higher, more borderline messages will be classified as spam.
		\end{solution}
	\end{parts}

	\question
	Consider a coin tossing experiment. You toss a coin 100 times, with the result of heads 70 times and tails 30 times. We denote the probability of heads of this coin as Î˜. Now consider a coin toss.
	\begin{parts}
		\part Build a model using maximum lilkelihood estimation (MLE) to infer $\Theta$.
		\begin{solution}
			Assume the coin has a probability of $\Theta$ to land in head. The probability of 70 heads in 100 tosses is:
			$$P(70 \mid \Theta, 100) = \binom{100}{70}\Theta^{70}(1-\Theta)^{30}$$
			We want to maximise this probability:
			\begin{equation*} 
		      \begin{split}
		      	\max_{0 \leq \Theta \leq 1}L(\Theta)\equiv &\max_{0 \leq \Theta \leq 1}P(70 \mid \Theta, 100)\\
		      	\equiv & \max_{0 \leq \Theta \leq 1}\binom{100}{70}\Theta^{70}(1-\Theta)^{30}\\
		      	\equiv & \max_{0 \leq \Theta \leq 1} \ln [\binom{100}{70}\Theta^{70}(1-\Theta)^{30}]\\
		      	\equiv & \max_{0 \leq \Theta \leq 1} [\ln \binom{100}{70} + \ln\Theta^{70} + \ln(1-\Theta)^{30})]\\
		      	\equiv & \max_{0 \leq \Theta \leq 1} [ 70 \ln\Theta+30 \ln(1-\Theta)] \Rightarrow 
		      		\frac{\mathrm{d}}{\mathrm{d} \Theta} [ 70 \ln\Theta+30 \ln(1-\Theta)]= 0 \\
		      	\equiv & \frac{70}{\Theta} + \frac{30}{1 - \Theta} = 0 \Rightarrow \Theta = 0.7\\
		      \end{split}
		    \end{equation*}
		\end{solution}
		\part Can we judge that this is an unfair coin? Explain your answer.
		\begin{solution}
			Yes, we can say this is an unfair coin with high confidence. \\
			Assume the coin is fair, i.e. $\Theta = 0.5$. The probability of 70 or more heads in 100 tosses is:
			$$ \frac{1}{2^{100}} \cdot (\binom{100}{70} + \binom{100}{71} + \cdots + \binom{100}{100}) \approx 3.925 \times 10^{-5}$$
			Which is very small, therefore the assumption of the coin is fair is very unlikely the case.
		\end{solution}
	\end{parts}

	\question
	In the programming logistic regression, part (c), we did away with the stochastic idea of SGD and substituted a round-robin version, which deterministically uses the next point in turn to perform the gradient descent. Describe whether you think this is a good robust idea or not for datasets in general.
	\begin{solution}
		No, random selection is still more robust.\\
		In a real world problem, the dataset might be sorted in some particular order, and it might cause
		the algorithm to stuck at a local minimum when a group of similar data are processed together.
		However, this problem is unlikely to occur if we are choosing the data randomly. \\
		Also, the result from deterministic gradient descent might be more suitable for the in sample data, but
		has relatively lower accuracy for predicting out sample data. If we choose randomly, it has similar
		effect of choosing random data globally, hence it will likely to 
		perform better on out sample data which matters more.
	\end{solution}
\end{questions}
\end{document}
