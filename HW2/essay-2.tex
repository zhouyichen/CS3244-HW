%% Based on a TeXnicCenter-Template by Gyorgy SZEIDL.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------
%
%\documentclass[10pt,a4paper]{exam}
\documentclass[11pt,a4paper,answers]{exam}
%
%----------------------------------------------------------
% This is a sample document for the standard LaTeX Book Class
% Class options
%       --  Body text point size:
%                        10pt (default), 11pt, 12pt
%       --  Paper size:  letterpaper (8.5x11 inch, default)
%                        a4paper, a5paper, b5paper,
%                        legalpaper, executivepaper
%       --  Orientation (portrait is the default):
%                        landscape
%       --  Printside:   oneside, twoside (default)
%       --  Quality:     final(default), draft
%       --  Title page:  titlepage, notitlepage
%       --  Columns:     onecolumn (default), twocolumn
%       --  Start chapter on left:
%                        openright(no, default), openany
%       --  Equation numbering (equation numbers on right is the default):
%                        leqno
%       --  Displayed equations (centered is the default):
%                        fleqn (flush left)
%       --  Open bibliography style (closed bibliography is the default):
%                        openbib
% For instance the command
%          \documentclass[a4paper,12pt,reqno]{book}
% ensures that the paper size is a4, fonts are typeset at the size 12p
% and the equation numbers are on the right side.
%
\usepackage{amsmath}%
\usepackage{amsthm}
\usepackage{amsfonts}%
\usepackage{amssymb}%
\usepackage{graphicx}
\usepackage{enumerate}
%----------------------------------------------------------
\newcommand{\IfSolution}[1]{#1}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}}
\renewcommand{\theenumiii}{\theenumii.\arabic{enumiii}}
\renewcommand{\qedsymbol}{$\blacksquare$}


\newtheorem{proposition}{Proposition}
\providecommand{\nline}{\vspace*{.5cm}}

\rhead{\textsf{CS3244-HW2}}
%\lhead{\textsf{Name: \underline{\hspace{1.2in}}}}


\global\long\def\qs{$\quad$}
\global\long\def\qss{$\quad\quad$}
\global\long\def\qsss{$\quad\quad\quad$}

%----------------------------------------------------------

\begin{document}

\begin{center}
{\bf CS3244 HW2}

{\bf A0113598X}
\end{center}

\nline

\begin{questions}
	\question
	Consider a binary classification problem. Let us denote the input set as \{$x_{n}, y_{n}$\} where $x_{n}$ is the data point and $y_{n} \in$ \{-1,+1\} is the corresponding class label. Further, misclassifying a positive class data point costs k times more than misclassifying a negative class data point. Rewrite the SVM optimization function to model this constraint.
	\begin{solution}
		Soft margin optimization function :\\
		Minimize $\frac{1}{2}\mathbf{w}^{T}\mathbf{w} + C \sum_{n=1}^{N}k^{\frac{y_{n} + 1}{2}}\xi _{n}$ \\
		Subject to $y_{n}(\mathbf{w}^{T}\mathbf{x} + b) \geq 1 - \xi_{n}$ and $\xi_{n} \geq 0$ for n = 1,2,...,N,
		$\mathbf{w} \in \mathbb{R}^{d}, b \in \mathbb{R}, \xi \in \mathbb{R}^{N}$ \\
		Lagrange Formulation:\\
		Maximise $ \mathcal{L}(\mathbf{\alpha}) = \sum_{n=1}^{N}\alpha_{n} - \frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}y_{n}y_{m}\alpha_{n}\alpha_{m}\mathbf{x}_{n}^{T}\mathbf{x}_{m}$\\
		w.r.t $\mathbf{\alpha} $
		subject to $0 \leq \alpha_{n} \leq C k^{\frac{y_{n} + 1}{2}}$ for n = 1,2,...,N and $\sum_{n=1}^{N}a_{n}y_{n} = 0$ \\
		$\Rightarrow \mathbf{w} =\sum_{n=1}^{N}\alpha_{n}y_{n}\mathbf{x}_{n} $ minimises $\frac{1}{2}\mathbf{w}^{T}\mathbf{w} + C \sum_{n=1}^{N}k^{\frac{y_{n} + 1}{2}}\xi _{n}$
	\end{solution}

	\question
	Consider the following training data (table omitted)
	\begin{parts}
		\part Plot these six training points. Are the classes \{+, âˆ’\} linearly separable?
		\begin{solution}
			\\
			\includegraphics[width=8cm, height=8cm]{figure_1}\\
			Yes they are linearly separable.
		\end{solution}
		\part Construct the weight vector of the maximum margin hyper-plane by inspection and identify the support vectors.
		\begin{solution}
			The support vectors are: (1, 1), (2, 0), (1, 0), and (0, 1). The maximum margin hyper-plane is $2x_{1} + 2x_{2} - 3 = 0$
		\end{solution}
		\part If you remove one of the support vectors, does the size of the optimal margin decrease, stay the same, or increase? Explain.
		\begin{solution}
			The margin stays the same. As there are two support vectors on both side of the separating plane. 
			By removing one of them, there is still at least one support vector left on one side, and two on the other side. So there 
			is always enough support vectors to determine the same margin as before.
		\end{solution}
		\part Is your answer to (c) also true for any dataset? Provide a counterexample or give a short proof.
		\begin{solution}
			No. \\
			Consider the dataset without the point (2, 0). The margin will be $2x_{1} + 2x_{2} - 3 = 0$\\
			If we remove one support vector (1, 1). Then the margin becomes $\frac{2}{3}x_{1} + \frac{2}{3}x_{2} - \frac{5}{3} = 0$.
			And the margin increases from $\sqrt{2}/4$ to $3\sqrt{2}/4$
		\end{solution}
	\end{parts}



\end{questions}
\end{document}
